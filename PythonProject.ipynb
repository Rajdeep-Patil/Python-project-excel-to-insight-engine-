{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729aed8f",
   "metadata": {},
   "source": [
    "***Problem Statement:***\n",
    "\n",
    "Organizations often receive large and complex Excel files containing multiple sheets of structured or semi-structured data. Manually exploring these files to understand column types, identify missing values, detect outliers, analyze categorical and date fields, and generate meaningful summaries can be time-consuming and error-prone.\n",
    "\n",
    "The objective is to develop an ***automated Excel Insights Engine*** that can:\n",
    "\n",
    "* Load Excel files with multiple sheets.\n",
    "\n",
    "* Provide essential metadata such as row and column counts, column names, and data types.\n",
    "\n",
    "* Detect and quantify missing values.\n",
    "\n",
    "* Generate summary statistics for numeric columns.\n",
    "\n",
    "* Identify strong correlations between numeric variables.\n",
    "\n",
    "* Detect outliers using statistical methods like IQR or Z-score.\n",
    "\n",
    "* Analyze categorical columns for frequency distributions and data quality.\n",
    "\n",
    "* Identify and summarize patterns in date columns, including day, month, and year distributions.\n",
    "\n",
    "* Automatically create visualizations to help users interpret the data.\n",
    "\n",
    "* Compile all findings into a structured report in markdown format or plain text.\n",
    "\n",
    "This tool will be useful for data analysts, auditors, business users, and domain experts who need to extract quick insights from Excel data without writing manual code every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6857723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "class ExcelInsights:\n",
    "    \"\"\"\n",
    "    A class to analyze Excel files and generate insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the ExcelInsights class.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str, optional): Path to the Excel file to analyze.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "        self.sheets = {}\n",
    "        self.insights = {}\n",
    "        \n",
    "    def load_excel(self, file_path=None):\n",
    "        \"\"\"\n",
    "        Load an Excel file into pandas DataFrames.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str, optional): Path to the Excel file to load.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise.\n",
    "        \"\"\"\n",
    "        if file_path:\n",
    "            self.file_path = file_path\n",
    "            \n",
    "        if not self.file_path:\n",
    "            print(\"Error: No file path provided.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Load all sheets from the Excel file\n",
    "            excel_file = pd.ExcelFile(self.file_path)\n",
    "            sheet_names = excel_file.sheet_names\n",
    "            \n",
    "            # Store each sheet as a DataFrame in the sheets dictionary\n",
    "            for sheet in sheet_names:\n",
    "                self.sheets[sheet] = pd.read_excel(self.file_path, sheet_name=sheet)\n",
    "                \n",
    "            # Set the first sheet as the default data\n",
    "            if sheet_names:\n",
    "                self.data = self.sheets[sheet_names[0]]\n",
    "                \n",
    "            print(f\"Successfully loaded {len(sheet_names)} sheets from {os.path.basename(self.file_path)}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Excel file: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_basic_info(self):\n",
    "        \"\"\"\n",
    "        Get basic information about the loaded data.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing basic information about the data.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"No data loaded. Please load an Excel file first.\")\n",
    "            return {}\n",
    "            \n",
    "        info = {\n",
    "            \"rows\": len(self.data),\n",
    "            \"columns\": len(self.data.columns),\n",
    "            \"column_names\": list(self.data.columns),\n",
    "            \"data_types\": {col: str(dtype) for col, dtype in self.data.dtypes.items()},\n",
    "            \"missing_values\": self.data.isnull().sum().to_dict(),\n",
    "            \"sheets\": list(self.sheets.keys())\n",
    "        }\n",
    "        \n",
    "        self.insights[\"basic_info\"] = info\n",
    "        return info\n",
    "    \n",
    "    def generate_summary_statistics(self, sheet_name=None):\n",
    "        \"\"\"\n",
    "        Generate summary statistics for numerical columns.\n",
    "        \n",
    "        Args:\n",
    "            sheet_name (str, optional): Name of the sheet to analyze. If None, uses the default data.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing summary statistics.\n",
    "        \"\"\"\n",
    "        if sheet_name and sheet_name in self.sheets:\n",
    "            data = self.sheets[sheet_name]\n",
    "        elif self.data is not None:\n",
    "            data = self.data\n",
    "        else:\n",
    "            print(\"No data loaded. Please load an Excel file first.\")\n",
    "            return {}\n",
    "        \n",
    "        # Get numerical columns\n",
    "        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if not numerical_cols:\n",
    "            print(\"No numerical columns found in the data.\")\n",
    "            return {}\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            \"numerical_columns\": numerical_cols,\n",
    "            \"statistics\": data[numerical_cols].describe().to_dict()\n",
    "        }\n",
    "        \n",
    "        # Add to insights\n",
    "        if \"summary_statistics\" not in self.insights:\n",
    "            self.insights[\"summary_statistics\"] = {}\n",
    "            \n",
    "        if sheet_name:\n",
    "            self.insights[\"summary_statistics\"][sheet_name] = summary\n",
    "        else:\n",
    "            self.insights[\"summary_statistics\"][\"default\"] = summary\n",
    "            \n",
    "        return summary\n",
    "    \n",
    "    def find_correlations(self, sheet_name=None, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Find correlations between numerical columns.\n",
    "        \n",
    "        Args:\n",
    "            sheet_name (str, optional): Name of the sheet to analyze. If None, uses the default data.\n",
    "            threshold (float, optional): Correlation threshold to report.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing correlations above the threshold.\n",
    "        \"\"\"\n",
    "        if sheet_name and sheet_name in self.sheets:\n",
    "            data = self.sheets[sheet_name]\n",
    "        elif self.data is not None:\n",
    "            data = self.data\n",
    "        else:\n",
    "            print(\"No data loaded. Please load an Excel file first.\")\n",
    "            return {}\n",
    "        \n",
    "        # Get numerical columns\n",
    "        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if len(numerical_cols) < 2:\n",
    "            print(\"Need at least 2 numerical columns to calculate correlations.\")\n",
    "            return {}\n",
    "        \n",
    "        # Calculate correlations\n",
    "        corr_matrix = data[numerical_cols].corr()\n",
    "        \n",
    "        # Find correlations above threshold\n",
    "        high_correlations = {}\n",
    "        for i in range(len(numerical_cols)):\n",
    "            for j in range(i+1, len(numerical_cols)):\n",
    "                col1 = numerical_cols[i]\n",
    "                col2 = numerical_cols[j]\n",
    "                correlation = corr_matrix.loc[col1, col2]\n",
    "                \n",
    "                if abs(correlation) >= threshold:\n",
    "                    high_correlations[f\"{col1} - {col2}\"] = correlation\n",
    "        \n",
    "        # Add to insights\n",
    "        if \"correlations\" not in self.insights:\n",
    "            self.insights[\"correlations\"] = {}\n",
    "            \n",
    "        if sheet_name:\n",
    "            self.insights[\"correlations\"][sheet_name] = high_correlations\n",
    "        else:\n",
    "            self.insights[\"correlations\"][\"default\"] = high_correlations\n",
    "            \n",
    "        return high_correlations\n",
    "    \n",
    "    def identify_outliers(self, sheet_name=None, method=\"iqr\", threshold=1.5):\n",
    "        \"\"\"\n",
    "        Identify outliers in numerical columns.\n",
    "        \n",
    "        Args:\n",
    "            sheet_name (str, optional): Name of the sheet to analyze. If None, uses the default data.\n",
    "            method (str, optional): Method to use for outlier detection ('iqr' or 'zscore').\n",
    "            threshold (float, optional): Threshold for outlier detection.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing outliers for each numerical column.\n",
    "        \"\"\"\n",
    "        if sheet_name and sheet_name in self.sheets:\n",
    "            data = self.sheets[sheet_name]\n",
    "        elif self.data is not None:\n",
    "            data = self.data\n",
    "        else:\n",
    "            print(\"No data loaded. Please load an Excel file first.\")\n",
    "            return {}\n",
    "        \n",
    "        # Get numerical columns\n",
    "        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if not numerical_cols:\n",
    "            print(\"No numerical columns found in the data.\")\n",
    "            return {}\n",
    "        \n",
    "        outliers = {}\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            col_data = data[col].dropna()\n",
    "            \n",
    "            if method == \"iqr\":\n",
    "                # IQR method\n",
    "                Q1 = col_data.quantile(0.25)\n",
    "                Q3 = col_data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "                \n",
    "                col_outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
    "                \n",
    "            elif method == \"zscore\":\n",
    "                # Z-score method\n",
    "                mean = col_data.mean()\n",
    "                std = col_data.std()\n",
    "                \n",
    "                if std == 0:  # Skip columns with zero standard deviation\n",
    "                    continue\n",
    "                    \n",
    "                z_scores = abs((col_data - mean) / std)\n",
    "                col_outliers = col_data[z_scores > threshold]\n",
    "                \n",
    "            else:\n",
    "                print(f\"Unknown method: {method}. Using IQR method instead.\")\n",
    "                Q1 = col_data.quantile(0.25)\n",
    "                Q3 = col_data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "                \n",
    "                col_outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
    "            \n",
    "            if not col_outliers.empty:\n",
    "                outliers[col] = {\n",
    "                    \"count\": len(col_outliers),\n",
    "                    \"percentage\": (len(col_outliers) / len(col_data)) * 100,\n",
    "                    \"values\": col_outliers.tolist() if len(col_outliers) <= 10 else col_outliers.tolist()[:10]  # Limit to 10 values\n",
    "                }\n",
    "        \n",
    "        # Add to insights\n",
    "        if \"outliers\" not in self.insights:\n",
    "            self.insights[\"outliers\"] = {}\n",
    "            \n",
    "        if sheet_name:\n",
    "            self.insights[\"outliers\"][sheet_name] = outliers\n",
    "        else:\n",
    "            self.insights[\"outliers\"][\"default\"] = outliers\n",
    "            \n",
    "        return outliers\n",
    "    \n",
    "    def analyze_categorical_data(self, sheet_name=None, top_n=5):\n",
    "        \"\"\"\n",
    "        Analyze categorical columns in the data.\n",
    "        \n",
    "        Args:\n",
    "            sheet_name (str, optional): Name of the sheet to analyze. If None, uses the default data.\n",
    "            top_n (int, optional): Number of top categories to include in the analysis.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing analysis of categorical columns.\n",
    "        \"\"\"\n",
    "        if sheet_name and sheet_name in self.sheets:\n",
    "            data = self.sheets[sheet_name]\n",
    "        elif self.data is not None:\n",
    "            data = self.data\n",
    "        else:\n",
    "            print(\"No data loaded. Please load an Excel file first.\")\n",
    "            return {}\n",
    "        \n",
    "        # Get categorical columns (object, string, or category dtype)\n",
    "        categorical_cols = data.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns.tolist()\n",
    "        \n",
    "        if not categorical_cols:\n",
    "            print(\"No categorical columns found in the data.\")\n",
    "            return {}\n",
    "        \n",
    "        categorical_analysis = {}\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            # Count value frequencies\n",
    "            value_counts = data[col].value_counts()\n",
    "            \n",
    "            # Get top N categories\n",
    "            top_categories = value_counts.head(top_n).to_dict()\n",
    "            \n",
    "            # Calculate percentage of total for each category\n",
    "            total_count = len(data[col].dropna())\n",
    "            top_categories_pct = {k: (v / total_count) * 100 for k, v in top_categories.items()}\n",
    "            \n",
    "            # Count unique values and missing values\n",
    "            unique_count = data[col].nunique()\n",
    "            missing_count = data[col].isnull().sum()\n",
    "            \n",
    "            categorical_analysis[col] = {\n",
    "                \"unique_values\": unique_count,\n",
    "                \"missing_values\": missing_count,\n",
    "                \"missing_percentage\": (missing_count / len(data)) * 100,\n",
    "                \"top_categories\": top_categories,\n",
    "                \"top_categories_pct\": top_categories_pct\n",
    "            }\n",
    "        \n",
    "        # Add to insights\n",
    "        if \"categorical_analysis\" not in self.insights:\n",
    "            self.insights[\"categorical_analysis\"] = {}\n",
    "            \n",
    "        if sheet_name:\n",
    "            self.insights[\"categorical_analysis\"][sheet_name] = categorical_analysis\n",
    "        else:\n",
    "            self.insights[\"categorical_analysis\"][\"default\"] = categorical_analysis\n",
    "            \n",
    "        return categorical_analysis\n",
    "    \n",
    "    def analyze_date_columns(self, sheet_name=None):\n",
    "        \"\"\"\n",
    "        Analyze date columns in the data.\n",
    "        \n",
    "        Args:\n",
    "            sheet_name (str, optional): Name of the sheet to analyze. If None, uses the default data.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing analysis of date columns.\n",
    "        \"\"\"\n",
    "        if sheet_name and sheet_name in self.sheets:\n",
    "            data = self.sheets[sheet_name]\n",
    "        elif self.data is not None:\n",
    "            data = self.data\n",
    "        else:\n",
    "            print(\"No data loaded. Please load an Excel file first.\")\n",
    "            return {}\n",
    "        \n",
    "        date_analysis = {}\n",
    "        \n",
    "        # Try to identify date columns\n",
    "        for col in data.columns:\n",
    "            # Check if column is already a datetime type\n",
    "            if pd.api.types.is_datetime64_any_dtype(data[col]):\n",
    "                date_col = data[col]\n",
    "            else:\n",
    "                # Try to convert to datetime\n",
    "                try:\n",
    "                    date_col = pd.to_datetime(data[col], errors='coerce')\n",
    "                    # If more than 70% of values could be converted to dates, consider it a date column\n",
    "                    if date_col.notnull().sum() / len(date_col) < 0.7:\n",
    "                        continue\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Analyze the date column\n",
    "            date_analysis[col] = {\n",
    "                \"min_date\": date_col.min().strftime('%Y-%m-%d') if not pd.isna(date_col.min()) else None,\n",
    "                \"max_date\": date_col.max().strftime('%Y-%m-%d') if not pd.isna(date_col.max()) else None,\n",
    "                \"range_days\": (date_col.max() - date_col.min()).days if not pd.isna(date_col.min()) and not pd.isna(date_col.max()) else None,\n",
    "                \"missing_values\": date_col.isnull().sum(),\n",
    "                \"missing_percentage\": (date_col.isnull().sum() / len(date_col)) * 100\n",
    "            }\n",
    "            \n",
    "            # Add day of week distribution if there are enough dates\n",
    "            if date_col.notnull().sum() > 10:\n",
    "                day_of_week = date_col.dt.day_name().value_counts().to_dict()\n",
    "                date_analysis[col][\"day_of_week_distribution\"] = day_of_week\n",
    "                \n",
    "                # Add month distribution\n",
    "                month_dist = date_col.dt.month_name().value_counts().to_dict()\n",
    "                date_analysis[col][\"month_distribution\"] = month_dist\n",
    "                \n",
    "                # Add year distribution\n",
    "                year_dist = date_col.dt.year.value_counts().to_dict()\n",
    "                date_analysis[col][\"year_distribution\"] = year_dist\n",
    "        \n",
    "        # Add to insights\n",
    "        if \"date_analysis\" not in self.insights:\n",
    "            self.insights[\"date_analysis\"] = {}\n",
    "            \n",
    "        if sheet_name:\n",
    "            self.insights[\"date_analysis\"][sheet_name] = date_analysis\n",
    "        else:\n",
    "            self.insights[\"date_analysis\"][\"default\"] = date_analysis\n",
    "            \n",
    "        return date_analysis\n",
    "    \n",
    "    def generate_insights_report(self, output_file=None):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive insights report.\n",
    "        \n",
    "        Args:\n",
    "            output_file (str, optional): Path to save the report. If None, returns the report as a string.\n",
    "            \n",
    "        Returns:\n",
    "            str: Report as a string if output_file is None, otherwise None.\n",
    "        \"\"\"\n",
    "        if not self.insights:\n",
    "            print(\"No insights generated. Please analyze the data first.\")\n",
    "            return None\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"# Excel Insights Report\")\n",
    "        report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Add file information\n",
    "        if self.file_path:\n",
    "            report.append(f\"## File Information\")\n",
    "            report.append(f\"- File: {os.path.basename(self.file_path)}\")\n",
    "            report.append(f\"- Path: {self.file_path}\")\n",
    "            if \"basic_info\" in self.insights:\n",
    "                info = self.insights[\"basic_info\"]\n",
    "                report.append(f\"- Sheets: {', '.join(info['sheets'])}\")\n",
    "                report.append(f\"- Rows: {info['rows']}\")\n",
    "                report.append(f\"- Columns: {info['columns']}\")\n",
    "                report.append(\"\")\n",
    "        \n",
    "        # Add summary statistics\n",
    "        if \"summary_statistics\" in self.insights:\n",
    "            report.append(\"## Summary Statistics\")\n",
    "            for sheet, stats in self.insights[\"summary_statistics\"].items():\n",
    "                if sheet != \"default\":\n",
    "                    report.append(f\"\\n### Sheet: {sheet}\")\n",
    "                \n",
    "                if not stats.get(\"numerical_columns\"):\n",
    "                    report.append(\"No numerical columns found for analysis.\")\n",
    "                    continue\n",
    "                \n",
    "                report.append(\"The following numerical columns were analyzed:\")\n",
    "                report.append(f\"- {', '.join(stats['numerical_columns'])}\\n\")\n",
    "                \n",
    "                for col, col_stats in stats[\"statistics\"].items():\n",
    "                    report.append(f\"### {col}\")\n",
    "                    report.append(f\"- Count: {col_stats.get('count', 'N/A')}\")\n",
    "                    report.append(f\"- Mean: {col_stats.get('mean', 'N/A'):.2f}\")\n",
    "                    report.append(f\"- Std Dev: {col_stats.get('std', 'N/A'):.2f}\")\n",
    "                    report.append(f\"- Min: {col_stats.get('min', 'N/A'):.2f}\")\n",
    "                    report.append(f\"- 25%: {col_stats.get('25%', 'N/A'):.2f}\")\n",
    "                    report.append(f\"- Median: {col_stats.get('50%', 'N/A'):.2f}\")\n",
    "                    report.append(f\"- 75%: {col_stats.get('75%', 'N/A'):.2f}\")\n",
    "                    report.append(f\"- Max: {col_stats.get('max', 'N/A'):.2f}\")\n",
    "                    report.append(\"\")\n",
    "        \n",
    "        # Add correlations\n",
    "        if \"correlations\" in self.insights:\n",
    "            report.append(\"## Correlations\")\n",
    "            for sheet, corrs in self.insights[\"correlations\"].items():\n",
    "                if sheet != \"default\":\n",
    "                    report.append(f\"\\n### Sheet: {sheet}\")\n",
    "                \n",
    "                if not corrs:\n",
    "                    report.append(\"No significant correlations found.\")\n",
    "                    continue\n",
    "                \n",
    "                report.append(\"The following pairs of columns show significant correlation:\")\n",
    "                for pair, corr in corrs.items():\n",
    "                    report.append(f\"- {pair}: {corr:.2f}\")\n",
    "                report.append(\"\")\n",
    "        \n",
    "        # Add outliers\n",
    "        if \"outliers\" in self.insights:\n",
    "            report.append(\"## Outliers\")\n",
    "            for sheet, outs in self.insights[\"outliers\"].items():\n",
    "                if sheet != \"default\":\n",
    "                    report.append(f\"\\n### Sheet: {sheet}\")\n",
    "                \n",
    "                if not outs:\n",
    "                    report.append(\"No outliers detected.\")\n",
    "                    continue\n",
    "                \n",
    "                for col, out_info in outs.items():\n",
    "                    report.append(f\"### {col}\")\n",
    "                    report.append(f\"- Outlier count: {out_info['count']}\")\n",
    "                    report.append(f\"- Percentage of data: {out_info['percentage']:.2f}%\")\n",
    "                    if out_info['values']:\n",
    "                        report.append(f\"- Sample outliers: {out_info['values']}\")\n",
    "                    report.append(\"\")\n",
    "        \n",
    "        # Add categorical analysis\n",
    "        if \"categorical_analysis\" in self.insights:\n",
    "            report.append(\"## Categorical Data Analysis\")\n",
    "            for sheet, cat_analysis in self.insights[\"categorical_analysis\"].items():\n",
    "                if sheet != \"default\":\n",
    "                    report.append(f\"\\n### Sheet: {sheet}\")\n",
    "                \n",
    "                if not cat_analysis:\n",
    "                    report.append(\"No categorical columns found for analysis.\")\n",
    "                    continue\n",
    "                \n",
    "                for col, col_analysis in cat_analysis.items():\n",
    "                    report.append(f\"### {col}\")\n",
    "                    report.append(f\"- Unique values: {col_analysis['unique_values']}\")\n",
    "                    report.append(f\"- Missing values: {col_analysis['missing_values']} ({col_analysis['missing_percentage']:.2f}%)\")\n",
    "                    \n",
    "                    report.append(\"\\nTop categories:\")\n",
    "                    for category, count in col_analysis['top_categories'].items():\n",
    "                        category_str = str(category) if category is not None else \"NULL\"\n",
    "                        pct = col_analysis['top_categories_pct'].get(category, 0)\n",
    "                        report.append(f\"- {category_str}: {count} ({pct:.2f}%)\")\n",
    "                    report.append(\"\")\n",
    "        \n",
    "        # Add date analysis\n",
    "        if \"date_analysis\" in self.insights:\n",
    "            report.append(\"## Date Analysis\")\n",
    "            for sheet, date_analysis in self.insights[\"date_analysis\"].items():\n",
    "                if sheet != \"default\":\n",
    "                    report.append(f\"\\n### Sheet: {sheet}\")\n",
    "                \n",
    "                if not date_analysis:\n",
    "                    report.append(\"No date columns found for analysis.\")\n",
    "                    continue\n",
    "                \n",
    "                for col, col_analysis in date_analysis.items():\n",
    "                    report.append(f\"### {col}\")\n",
    "                    report.append(f\"- Date range: {col_analysis['min_date']} to {col_analysis['max_date']}\")\n",
    "                    report.append(f\"- Range in days: {col_analysis['range_days']}\")\n",
    "                    report.append(f\"- Missing values: {col_analysis['missing_values']} ({col_analysis['missing_percentage']:.2f}%)\")\n",
    "                    \n",
    "                    if \"day_of_week_distribution\" in col_analysis:\n",
    "                        report.append(\"\\nDay of week distribution:\")\n",
    "                        for day, count in col_analysis['day_of_week_distribution'].items():\n",
    "                            report.append(f\"- {day}: {count}\")\n",
    "                    \n",
    "                    if \"month_distribution\" in col_analysis:\n",
    "                        report.append(\"\\nMonth distribution:\")\n",
    "                        for month, count in col_analysis['month_distribution'].items():\n",
    "                            report.append(f\"- {month}: {count}\")\n",
    "                    \n",
    "                    if \"year_distribution\" in col_analysis:\n",
    "                        report.append(\"\\nYear distribution:\")\n",
    "                        for year, count in col_analysis['year_distribution'].items():\n",
    "                            report.append(f\"- {year}: {count}\")\n",
    "                    report.append(\"\")\n",
    "        \n",
    "        # Compile the report\n",
    "        report_text = \"\\n\".join(report)\n",
    "        \n",
    "        # Save to file if specified\n",
    "        if output_file:\n",
    "            try:\n",
    "                with open(output_file, 'w') as f:\n",
    "                    f.write(report_text)\n",
    "                print(f\"Report saved to {output_file}\")\n",
    "                return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving report: {e}\")\n",
    "                return report_text\n",
    "        \n",
    "        return report_text\n",
    "    \n",
    "    def visualize_data(self, output_dir=None, sheet_name=None):\n",
    "        \"\"\"\n",
    "        Generate visualizations for the data.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str, optional): Directory to save visualizations. If None, displays plots.\n",
    "            sheet_name (str, optional): Name of the sheet to visualize. If None, uses the default data.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of paths to saved visualizations if output_dir is provided, otherwise None.\n",
    "        \"\"\"\n",
    "        if sheet_name and sheet_name in self.sheets:\n",
    "            data = self.sheets[sheet_name]\n",
    "        elif self.data is not None:\n",
    "            data = self.data\n",
    "        else:\n",
    "            print(\"No data loaded. Please load an Excel file first.\")\n",
    "            return []\n",
    "        \n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        saved_files = []\n",
    "        \n",
    "        # Get numerical and categorical columns\n",
    "        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = data.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns.tolist()\n",
    "        \n",
    "        # 1. Histograms for numerical columns\n",
    "        for col in numerical_cols[:5]:  # Limit to first 5 columns to avoid too many plots\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(data[col].dropna(), bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            if output_dir:\n",
    "                file_path = os.path.join(output_dir, f\"histogram_{col}.png\")\n",
    "                plt.savefig(file_path)\n",
    "                saved_files.append(file_path)\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "        \n",
    "        # 2. Bar charts for categorical columns\n",
    "        for col in categorical_cols[:5]:  # Limit to first 5 columns\n",
    "            # Get top 10 categories\n",
    "            value_counts = data[col].value_counts().head(10)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            bars = plt.bar(value_counts.index.astype(str), value_counts.values, color='lightgreen', edgecolor='black')\n",
    "            plt.title(f'Top 10 Categories in {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add count labels on top of bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                        f'{height}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if output_dir:\n",
    "                file_path = os.path.join(output_dir, f\"barchart_{col}.png\")\n",
    "                plt.savefig(file_path)\n",
    "                saved_files.append(file_path)\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "        \n",
    "        # 3. Correlation heatmap for numerical columns\n",
    "        if len(numerical_cols) > 1:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            corr_matrix = data[numerical_cols].corr()\n",
    "            plt.imshow(corr_matrix, cmap='coolwarm', interpolation='none', aspect='auto')\n",
    "            plt.colorbar(label='Correlation Coefficient')\n",
    "            plt.title('Correlation Heatmap')\n",
    "            \n",
    "            # Add correlation values\n",
    "            for i in range(len(numerical_cols)):\n",
    "                for j in range(len(numerical_cols)):\n",
    "                    plt.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
    "                             ha='center', va='center', \n",
    "                             color='white' if abs(corr_matrix.iloc[i, j]) > 0.5 else 'black')\n",
    "            \n",
    "            plt.xticks(range(len(numerical_cols)), numerical_cols, rotation=45, ha='right')\n",
    "            plt.yticks(range(len(numerical_cols)), numerical_cols)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if output_dir:\n",
    "                file_path = os.path.join(output_dir, \"correlation_heatmap.png\")\n",
    "                plt.savefig(file_path)\n",
    "                saved_files.append(file_path)\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "        \n",
    "        # 4. Box plots for numerical columns to visualize outliers\n",
    "        if numerical_cols:\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            data[numerical_cols[:10]].boxplot()  # Limit to first 10 columns\n",
    "            plt.title('Box Plots for Numerical Columns')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if output_dir:\n",
    "                file_path = os.path.join(output_dir, \"boxplots.png\")\n",
    "                plt.savefig(file_path)\n",
    "                saved_files.append(file_path)\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "        \n",
    "        # 5. Pie charts for categorical columns with few unique values\n",
    "        for col in categorical_cols[:3]:  # Limit to first 3 columns\n",
    "            value_counts = data[col].value_counts()\n",
    "            \n",
    "            # Only create pie chart if there are 10 or fewer unique values\n",
    "            if len(value_counts) <= 10:\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.pie(value_counts.values, labels=value_counts.index.astype(str), \n",
    "                        autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "                plt.title(f'Distribution of {col}')\n",
    "                plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "                \n",
    "                if output_dir:\n",
    "                    file_path = os.path.join(output_dir, f\"piechart_{col}.png\")\n",
    "                    plt.savefig(file_path)\n",
    "                    saved_files.append(file_path)\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "        \n",
    "        return saved_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e89262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3 sheets from sample_data.xlsx\n",
      "## Basic Information\n"
     ]
    }
   ],
   "source": [
    "file_path = 'samples/sample_data.xlsx'\n",
    "excel_insights=ExcelInsights(file_path)\n",
    "\n",
    "excel_insights.load_excel()\n",
    "print('## Basic Information')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcabcf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows : 366\n",
      "columns : 16\n",
      "column_names : ['Date', 'Product', 'Category', 'Region', 'Units_Sold', 'Unit_Price', 'Shipping_Cost', 'Discount_Pct', 'Customer_Rating', 'Returns', 'Total_Sales', 'Month', 'Day_of_Week', 'Is_Weekend', 'Profit', 'Profit_Margin']\n",
      "data_types : {'Date': 'datetime64[ns]', 'Product': 'object', 'Category': 'object', 'Region': 'object', 'Units_Sold': 'int64', 'Unit_Price': 'float64', 'Shipping_Cost': 'float64', 'Discount_Pct': 'float64', 'Customer_Rating': 'float64', 'Returns': 'int64', 'Total_Sales': 'float64', 'Month': 'object', 'Day_of_Week': 'object', 'Is_Weekend': 'bool', 'Profit': 'float64', 'Profit_Margin': 'float64'}\n",
      "missing_values : {'Date': 0, 'Product': 0, 'Category': 0, 'Region': 0, 'Units_Sold': 0, 'Unit_Price': 0, 'Shipping_Cost': 15, 'Discount_Pct': 17, 'Customer_Rating': 23, 'Returns': 0, 'Total_Sales': 0, 'Month': 0, 'Day_of_Week': 0, 'Is_Weekend': 0, 'Profit': 15, 'Profit_Margin': 15}\n",
      "sheets : ['Sales', 'Employees', 'Customers']\n",
      "### Summary Statistics\n",
      "numerical_columns : ['Units_Sold', 'Unit_Price', 'Shipping_Cost', 'Discount_Pct', 'Customer_Rating', 'Returns', 'Total_Sales', 'Profit', 'Profit_Margin']\n",
      "statistics : {'Units_Sold': {'count': 366.0, 'mean': 27.47814207650273, 'std': 20.22756326013523, 'min': 1.0, '25%': 14.0, '50%': 27.0, '75%': 38.0, 'max': 174.0}, 'Unit_Price': {'count': 366.0, 'mean': 522.6271584699452, 'std': 328.8882647054234, 'min': 10.24, '25%': 240.26999999999998, '50%': 527.25, '75%': 762.23, 'max': 1996.29}, 'Shipping_Cost': {'count': 351.0, 'mean': 27.370028490028492, 'std': 12.22020005678129, 'min': 5.06, '25%': 16.745, '50%': 28.13, '75%': 37.245000000000005, 'max': 49.8}, 'Discount_Pct': {'count': 349.0, 'mean': 10.673352435530086, 'std': 7.165282313030815, 'min': 0.0, '25%': 5.0, '50%': 10.0, '75%': 15.0, 'max': 20.0}, 'Customer_Rating': {'count': 343.0, 'mean': 2.988338192419825, 'std': 1.389132162896395, 'min': 1.0, '25%': 2.0, '50%': 3.0, '75%': 4.0, 'max': 5.0}, 'Returns': {'count': 366.0, 'mean': 0.05737704918032787, 'std': 0.2328800147716333, 'min': 0.0, '25%': 0.0, '50%': 0.0, '75%': 0.0, 'max': 1.0}, 'Total_Sales': {'count': 366.0, 'mean': 11701.770355191258, 'std': 10136.923608265966, 'min': 50.83, '25%': 3326.075, '50%': 9243.625, '75%': 17944.0275, 'max': 42446.25}, 'Profit': {'count': 351.0, 'mean': 11640.239458689459, 'std': 10144.27896892597, 'min': 15.82, '25%': 3332.13, '50%': 9108.4, '75%': 17907.175, 'max': 42416.84}, 'Profit_Margin': {'count': 351.0, 'mean': 98.4385754985755, 'std': 5.7065752771315825, 'min': 31.12, '25%': 99.065, '50%': 99.74, '75%': 99.86, 'max': 99.98}}\n",
      "\n",
      "\n",
      "Correlation\n",
      "Unit_Price - Total_Sales : 0.621514807163946\n",
      "Unit_Price - Profit : 0.611650836560738\n",
      "Total_Sales - Profit : 0.9999992753244941\n",
      "##Outliers\n",
      "Units_Sold : {'count': 5, 'percentage': 1.366120218579235, 'values': [174, 136, 157, 167, 121]}\n",
      "Unit_Price : {'count': 4, 'percentage': 1.092896174863388, 'values': [1635.32, 1578.1, 1994.21, 1996.29]}\n",
      "Returns : {'count': 21, 'percentage': 5.737704918032787, 'values': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Total_Sales : {'count': 3, 'percentage': 0.819672131147541, 'values': [40087.61, 42446.25, 42241.8]}\n",
      "Profit : {'count': 3, 'percentage': 0.8547008547008548, 'values': [40058.05, 42416.84, 42213.1]}\n",
      "Profit_Margin : {'count': 41, 'percentage': 11.68091168091168, 'values': [97.54, 97.56, 88.88, 78.65, 95.25, 92.11, 96.41, 93.78, 91.5, 97.58]}\n",
      "\n",
      "\n",
      " Categorical analysis\n",
      "Product : {'unique_values': 8, 'missing_values': 0, 'missing_percentage': 0.0, 'top_categories': {'Smartphone': 56, 'Keyboard': 56, 'Monitor': 48, 'Laptop': 47, 'Printer': 44}, 'top_categories_pct': {'Smartphone': 15.300546448087433, 'Keyboard': 15.300546448087433, 'Monitor': 13.114754098360656, 'Laptop': 12.841530054644808, 'Printer': 12.021857923497267}}\n",
      "Category : {'unique_values': 3, 'missing_values': 0, 'missing_percentage': 0.0, 'top_categories': {'Electronics': 127, 'Peripherals': 120, 'Accessories': 119}, 'top_categories_pct': {'Electronics': 34.69945355191257, 'Peripherals': 32.78688524590164, 'Accessories': 32.51366120218579}}\n",
      "Region : {'unique_values': 5, 'missing_values': 0, 'missing_percentage': 0.0, 'top_categories': {'Central': 79, 'North': 76, 'East': 72, 'South': 71, 'West': 68}, 'top_categories_pct': {'Central': 21.584699453551913, 'North': 20.76502732240437, 'East': 19.672131147540984, 'South': 19.398907103825135, 'West': 18.579234972677597}}\n",
      "Month : {'unique_values': 12, 'missing_values': 0, 'missing_percentage': 0.0, 'top_categories': {'October': 38, 'January': 38, 'July': 36, 'September': 35, 'May': 32}, 'top_categories_pct': {'October': 10.382513661202186, 'January': 10.382513661202186, 'July': 9.836065573770492, 'September': 9.562841530054644, 'May': 8.743169398907105}}\n",
      "Day_of_Week : {'unique_values': 7, 'missing_values': 0, 'missing_percentage': 0.0, 'top_categories': {'Monday': 67, 'Saturday': 53, 'Sunday': 53, 'Tuesday': 51, 'Wednesday': 49}, 'top_categories_pct': {'Monday': 18.30601092896175, 'Saturday': 14.48087431693989, 'Sunday': 14.48087431693989, 'Tuesday': 13.934426229508196, 'Wednesday': 13.387978142076504}}\n",
      "\n",
      "\n",
      "Date Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9852\\2983488814.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_col = pd.to_datetime(data[col], errors='coerce')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9852\\2983488814.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_col = pd.to_datetime(data[col], errors='coerce')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9852\\2983488814.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_col = pd.to_datetime(data[col], errors='coerce')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9852\\2983488814.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_col = pd.to_datetime(data[col], errors='coerce')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9852\\2983488814.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_col = pd.to_datetime(data[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date : {'min_date': '2024-06-03', 'max_date': '2025-06-02', 'range_days': 364, 'missing_values': 0, 'missing_percentage': 0.0, 'day_of_week_distribution': {'Monday': 67, 'Saturday': 53, 'Sunday': 53, 'Tuesday': 51, 'Wednesday': 49, 'Thursday': 47, 'Friday': 46}, 'month_distribution': {'October': 38, 'January': 38, 'July': 36, 'September': 35, 'May': 32, 'December': 31, 'November': 31, 'February': 29, 'April': 29, 'March': 28, 'June': 24, 'August': 15}, 'year_distribution': {2024: 208, 2025: 158}}\n",
      "Units_Sold : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 0, 'missing_percentage': 0.0, 'day_of_week_distribution': {'Thursday': 366}, 'month_distribution': {'January': 366}, 'year_distribution': {1970: 366}}\n",
      "Unit_Price : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 0, 'missing_percentage': 0.0, 'day_of_week_distribution': {'Thursday': 366}, 'month_distribution': {'January': 366}, 'year_distribution': {1970: 366}}\n",
      "Shipping_Cost : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 15, 'missing_percentage': 4.098360655737705, 'day_of_week_distribution': {'Thursday': 351}, 'month_distribution': {'January': 351}, 'year_distribution': {1970.0: 351}}\n",
      "Discount_Pct : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 17, 'missing_percentage': 4.644808743169399, 'day_of_week_distribution': {'Thursday': 349}, 'month_distribution': {'January': 349}, 'year_distribution': {1970.0: 349}}\n",
      "Customer_Rating : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 23, 'missing_percentage': 6.284153005464481, 'day_of_week_distribution': {'Thursday': 343}, 'month_distribution': {'January': 343}, 'year_distribution': {1970.0: 343}}\n",
      "Returns : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 0, 'missing_percentage': 0.0, 'day_of_week_distribution': {'Thursday': 366}, 'month_distribution': {'January': 366}, 'year_distribution': {1970: 366}}\n",
      "Total_Sales : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 0, 'missing_percentage': 0.0, 'day_of_week_distribution': {'Thursday': 366}, 'month_distribution': {'January': 366}, 'year_distribution': {1970: 366}}\n",
      "Profit : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 15, 'missing_percentage': 4.098360655737705, 'day_of_week_distribution': {'Thursday': 351}, 'month_distribution': {'January': 351}, 'year_distribution': {1970.0: 351}}\n",
      "Profit_Margin : {'min_date': '1970-01-01', 'max_date': '1970-01-01', 'range_days': 0, 'missing_values': 15, 'missing_percentage': 4.098360655737705, 'day_of_week_distribution': {'Thursday': 351}, 'month_distribution': {'January': 351}, 'year_distribution': {1970.0: 351}}\n",
      "\n",
      "\n",
      " Generating Insights Report:\n",
      "Report saved to insights_report.txt\n",
      "Report generated and saved to insights_report.text\n",
      "visualization saved:\n",
      "visualization\\histogram_Units_Sold.png\n",
      "visualization\\histogram_Unit_Price.png\n",
      "visualization\\histogram_Shipping_Cost.png\n",
      "visualization\\histogram_Discount_Pct.png\n",
      "visualization\\histogram_Customer_Rating.png\n",
      "visualization\\barchart_Product.png\n",
      "visualization\\barchart_Category.png\n",
      "visualization\\barchart_Region.png\n",
      "visualization\\barchart_Month.png\n",
      "visualization\\barchart_Day_of_Week.png\n",
      "visualization\\correlation_heatmap.png\n",
      "visualization\\boxplots.png\n",
      "visualization\\piechart_Product.png\n",
      "visualization\\piechart_Category.png\n",
      "visualization\\piechart_Region.png\n"
     ]
    }
   ],
   "source": [
    "basic_info = excel_insights.get_basic_info()\n",
    "for key,value in basic_info.items():\n",
    "    print(f'{key} : {value}')\n",
    "print(\"### Summary Statistics\")\n",
    "summary_statistics=excel_insights.generate_summary_statistics()\n",
    "for key,value in summary_statistics.items():\n",
    "    print(f'{key} : {value}')\n",
    "\n",
    "print('\\n\\nCorrelation')\n",
    "correlation=excel_insights.find_correlations()\n",
    "for key,value in correlation.items():\n",
    "    print(f'{key} : {value}')\n",
    "    \n",
    "print('##Outliers')\n",
    "outliers=excel_insights.identify_outliers()\n",
    "for key,value in outliers.items():\n",
    "    print(f'{key} : {value}')\n",
    "    \n",
    "print('\\n\\n Categorical analysis')\n",
    "categorical_analysis=excel_insights.analyze_categorical_data()\n",
    "for key,value in categorical_analysis.items():\n",
    "    print(f'{key} : {value}')\n",
    "    \n",
    "print('\\n\\nDate Analysis')\n",
    "date_analysis=excel_insights.analyze_date_columns()\n",
    "for key,value in date_analysis.items():\n",
    "    print(f'{key} : {value}')\n",
    "    \n",
    "print('\\n\\n Generating Insights Report:')\n",
    "report = excel_insights.generate_insights_report(output_file='insights_report.txt')\n",
    "\n",
    "if report:\n",
    "    print(report)\n",
    "else:\n",
    "    print('Report generated and saved to insights_report.text')\n",
    "\n",
    "visualizations = excel_insights.visualize_data(output_dir='visualization')\n",
    "if visualizations:\n",
    "    print('visualization saved:')\n",
    "    for file in visualizations:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13078fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
